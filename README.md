# TGP-TðŸš€: Compound Text-Guided Prompt Tuning via Image-Adaptive Cues

Official implementation of the paper in AAAI 2024:

**Compound Text-Guided Prompt Tuning via Image-Adaptive Cues**

[Hao Tan](https://scholar.google.com/citations?hl=zh-CN&user=gPEjNFcAAAAJ), [Jun Li](https://bazinga699.github.io/), [Yizhuang Zhou](https://scholar.google.com/citations?user=VRSGDDEAAAAJ), [Jun Wan](http://www.cbsr.ia.ac.cn/users/jwan/), [Zhen Lei](http://www.cbsr.ia.ac.cn/users/zlei/), [Xiangyu Zhang](https://scholar.google.com/citations?user=yuB-cfoAAAAJ)

[[PDF](https://arxiv.org/pdf/2312.06401.pdf)]

<p align="center">
    <img src="src/paradigm_comparison.gif" alt="TGP-T Paradigm">
</p>


## IntroductionðŸ§­

TGP-T is an efficient prompt tuning framework for adapting VLMs with significantly lower resource demand. We introduce compound text supervision to guide the optimization of prompts, i.e., category-wise and content-wise text supervision. Through a Bonder structure, we align the generated prompts with visual features. As a result, we only need two prompt inputs to text encoder to produce state-of-the-art performance on 11 datasets for few-shot classification.

<p align="center">
    <img src="src/method.png" alt="TGP-T Framework">
</p>


## RequirementsðŸ“¨

### Installation

We recommend to install the environment through `conda` and `pip`.

```bash
conda create -n tgpt python=3.8
conda activate tgpt

# Install the dependencies
pip install -r requirements.txt
```

### Dataset

Follow these steps to prepare the datasets: 

#### 1. Images
- Please follow `data/DATASETS.md` to download the 11 datasets along with `ImageNetV2` and `ImageNet-Sketch`. 

#### 2. Content descriptions

- Download all the content descriptions and preprocessed data files in [GoogleDrive](https://drive.google.com/file/d/1OA3lL9wJ8p7sGSm_JII_5RNyS_jwOlCM/view?usp=sharing) or [Baidu Netdisk](https://pan.baidu.com/s/1FeCS4A-_ai8LtMitrmeDLw?pwd=ky6d) (passward:ky6d).
- We provide the content descriptions generated by `MiniGPT-4` for all 11 datasets. The statistics are shown in the following table. 
- Each line in the `descriptions.txt` contains three elements separated by `\t`, i.e., image name, content description and category of the image.

| Dataset | # Descriptions | Avg. Sentence Length  | Example | Description |
|---------|:----------------:|:----------------------:|:---------:|:-------------:|
| Caltech101 | 8,242          | 20.04                | <img src="src/examples/caltech/image_0206.jpg" alt="Caltech Image" width="100" height="50"> |[Faces] The man in the image has a bald head and a scruffy beard. |
| DTD | 5,640          | 17.64             | <img src="src/examples/dtd/studded_0134.jpg" alt="Caltech Image" width="100" height="60"> |[Studded] The couch has silver studs on the armrests and legs. |
| EuroSAT | 27,000         | 19.81           | <img src="src/examples/eurosat/pasture_96.jpg" alt="Caltech Image" width="100" height="60"> |[Pasture] The pasture land in this image is an open field with green grass and dotted with small trees and bushes. |
| FGVCAircraft | 3,334        | 21.43         | <img src="src/examples/aircraft/0829997.jpg" alt="Caltech Image" width="100" height="60"> |[A320] The A320 is a white airplane with red and white stripes and the German flag on the tail. |
| Food101 | 101,000         | 21.34            | <img src="src/examples/food/2515221.jpg" alt="Caltech Image" width="100" height="60"> |[Hot dog] This hot dog has chili and cheese on it. |
| ImageNet | 90,053         | 22.03           | <img src="src/examples/imagenet/n01498041_3238.JPEG" alt="Caltech Image" width="100" height="60"> |[Stingray] The stingray in the image is a large, majestic marine animal with a long, slender body and wide wings. |
| StanfordCars | 8,144       | 21.67          | <img src="src/examples/cars/00013.jpg" alt="Caltech Image" width="100" height="50"> |[2007 Hyundai Elantra Sedan] The 2007 Hyundai Elantra Sedan is a sleek and stylish silver car on display at an auto show. |
| OxfordPets | 3,680          | 22.34             | <img src="src/examples/pets/Ragdoll_141.jpg" alt="Caltech Image" width="100" height="80"> |[Ragdoll] The ragdoll cat in the image has blue eyes and a gray and white body with soft, fluffy fur. |
| Flowers102 | 8,189          | 18.11             | <img src="src/examples/flowers/image_00128.jpg" alt="Caltech Image" width="100" height="60"> |[Passion flower] The passion flower is a beautiful purple flower with white stripes and a long stem. |
| UCF101 | 7,639          | 19.62             | <img src="src/examples/ucf/v_BasketballDunk_g14.jpg" alt="Caltech Image" width="100" height="60"> |[Basketball_Dunk] This image shows a basketball player dunking the ball over an opponent during a game. |
| SUN397 | 19,850        | 23.00             | <img src="src/examples/sun397/sun_adbjrupeylqaocyb.jpg" alt="Caltech Image" width="100" height="60"> |[Hospital_room] The hospital room has several beds, a desk, and modern medical equipment. |

#### 3. Data organization
- Put them in the same directory. For example, the directory structure should look like:

```
imagenet/

|-- descriptions.txt

|-- images/

|   |-- train/ # contains 1,000 folders like n01440764, n01443537, etc.

|   |-- val/

|-- split_fewshot/

|   |-- shot_16-seed_1.pkl  # shot_16-seed_2.pkl, shot_8-seed_1.pkl, etc.
```

Before training, make sure you change the image paths in the `split_fewshot/shot_{x}-seed_{x}.pkl`. 

We provide `tools/convert_path.py`  to get this done. To trigger convertion for all datasets, you can run this simple command:

```shell
sh convert_path.sh [/root/path/to/your/data]

# For example
# sh convert_path.sh ./recognition/data
```



## UsageðŸ§©

### 1. Configs

The running configurations can be modified in `configs/configs/dataset.yaml`, including number of shots, visual encoders, and hyperparamters.

For simplicity, we provide the hyperparamters achieving the overall best performance on 16 shots for a dataset, which is accord with the scores in our paper. If respectively tuned for different shot numbers, the 1~16-shot performance can be further improved. You can edit the `MAX_ITER`, `LR` for fine-grained tuning.

### 2. Running

#### Few-shot RecognitionðŸŽ¯

**Training.** To train on a specific dataset, all you need is `train.py` :

- Specify the dataset configs by `--config-file`
- Specify your path to the dataset by `--root`
- Specify the path where you want to save the results (inlcuding training logs and weights) by `--output-dir`
- You can turn on the logging on `wandb ` by enabling `--use-wandb`. To specify the project name of wandb, you can use `--wandb-proj [your-proj-name]`

For example, to run on ImageNet:

```shell
python train.py --config-file configs/configs/imagenet.yaml --use-wandb
```

**Reproduction.** We also provide an easy way `main.sh` to reproduce the results in our paper. This will trigger 16-shot training on all 11 datasets, including different seeds. To speed up the training, you can choose to parallel them on multiple GPUs.

```shell
sh main.sh
```

#### Distribution ShiftðŸŽ¯

To perform distribution shift experiments, all you need is `train_xdomain.py`. We take "Source" dataset as `ImageNet`, "Target" datasets as `ImageNet-V2` and `ImageNet-Sketch`. 

Running the following command will automatically load the weights you have trained on 16-shot ImageNet. If you haven't trained on ImageNet before, it will first start training and then evaluation.

```shell
python train_xdomain.py --use-wandb
```



## DiscussionðŸ’¬

To inject rich semantic knowledge into prompts, we take advantages of `MiniGPT-4` to generate the content descriptions. Here are some related discussions:

- In order to reduce noise from `MiniGPT-4`, i.e., focus on the target object rather than background information, we adjust the input prompts for `MiniGPT-4` and avoid the model from generating overly long sentences. After testing, we choose `Describe the {classname} in this image in one sentence.` as the final input prompts.
- `MiniGPT-4` is introduced only during the training phase and does not cause information leakage in the test phase. Actually, the enhancement brought to visual tasks by general knowledge of large models is indeed highly interesting. We also hope this work can inspire more resource-efficient ways on this.



## Citation

```
@article{tan2023compound,
  title={Compound Text-Guided Prompt Tuning via Image-Adaptive Cues},
  author={Tan, Hao and Li, Jun and Zhou, Yizhuang and Wan, Jun and Lei, Zhen and Zhang, Xiangyu},
  journal={arXiv preprint arXiv:2312.06401},
  year={2023}
}
```



## Acknowledgements

This repo benefits from [CLIP](https://github.com/openai/CLIP), [CoOp](https://github.com/KaiyangZhou/Dassl.pytorch) and [Cross-Modal Adaptation](https://github.com/linzhiqiu/cross_modal_adaptation). Thanks for their wonderful works.
